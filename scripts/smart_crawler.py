#!/usr/bin/env python
"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø®Ø²Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø®Ø²Ø´Ú¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ù‚ÙˆÙ‚ÛŒ

Ø§ÛŒÙ† Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¨Ù‡ ØµÙˆØ±Øª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ùˆ Ø®ÙˆØ¯Ú©Ø§Ø±ØŒ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ Ø±Ø§ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ø±Ø¯Ù‡ØŒ
Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ùˆ Ø¨Ø§ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ØŒ Ø®Ø²Ø´ Ø³Ø§ÛŒØª Ù‡Ø¯Ù Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
ØªÙ†Ø¸ÛŒÙ… ÙÙˆØ§ØµÙ„ Ø²Ù…Ø§Ù†ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª ØªØ·Ø¨ÛŒÙ‚ÛŒ Ø§Ø² Ú©ÙˆØªØ§Ù‡ (Ø¯Ø± Ø´Ø±ÙˆØ¹) Ø¨Ù‡ Ø·ÙˆÙ„Ø§Ù†ÛŒ (Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡) Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.
"""

import os
import sys
import time
import random
import signal
import argparse
import threading
import queue
import json
from datetime import datetime
from typing import Dict, List, Optional, Any, Union
from urllib.parse import urlparse

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ
from dotenv import load_dotenv

load_dotenv()

# Ø§ÙØ²ÙˆØ¯Ù† Ù…Ø³ÛŒØ± Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ù‡ Ø³ÛŒØ³ØªÙ… (ÙÙ‚Ø· Ø§Ú¯Ø± Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¨Ø§Ø´Ø¯)
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.append(project_root)

# ÙˆØ§Ø±Ø¯Ø³Ø§Ø²ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾Ø±ÙˆÚ˜Ù‡
from config.settings import (
    DB_CONFIG, CRAWLER_CONFIG, BASE_DIR, DATA_DIR, LOGS_DIR,
    load_defaults, load_domain_config, get_user_agent_list
)

# ÙˆØ§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡
from database.connection import DatabaseConnection
from database.schema import create_tables

# ÙˆØ§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ù‡Ø³ØªÙ‡ Ø®Ø²Ø´Ú¯Ø±
from core.crawler import Crawler, CrawlState
from core.structure_discovery import StructureDiscovery
from core.content_extractor import ContentExtractor
from core.classifier import TextClassifier
from core.storage import StorageManager

# ÙˆØ§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§
from models.content import ContentItem

# ÙˆØ§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ú©Ù…Ú©ÛŒ
from utils.logger import get_logger, get_crawler_logger
from utils.text import extract_links

# ØªÙ†Ø¸ÛŒÙ… Ù„Ø§Ú¯Ø±Ù‡Ø§
logger = get_logger("smart_crawler")
crawler_logger = get_crawler_logger()

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶
DEFAULT_CONFIG = load_defaults()


class SmartCrawlManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø®Ø²Ø´ Ø¨Ø§ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø²Ù…Ø§Ù†ÛŒ ØªØ·Ø¨ÛŒÙ‚ÛŒ"""

    def __init__(self, base_url: str, config_dir: str = None,
                 max_threads: int = None, initial_delay: float = None,
                 respect_robots: bool = None, database_retry_attempts: int = 5):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù…Ø¯ÛŒØ±ÛŒØª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø®Ø²Ø´

        Args:
            base_url: Ø¢Ø¯Ø±Ø³ Ù¾Ø§ÛŒÙ‡ ÙˆØ¨Ø³Ø§ÛŒØª Ù‡Ø¯Ù
            config_dir: Ù…Ø³ÛŒØ± Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
            max_threads: Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ù†Ø®â€ŒÙ‡Ø§ÛŒ Ù‡Ù…Ø²Ù…Ø§Ù† (Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª)
            initial_delay: ØªØ£Ø®ÛŒØ± Ø§ÙˆÙ„ÛŒÙ‡ Ø¨ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ (Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª)
            respect_robots: Ø¢ÛŒØ§ Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ robots.txt Ø±Ø¹Ø§ÛŒØª Ø´ÙˆØ¯ØŸ (Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª)
            database_retry_attempts: ØªØ¹Ø¯Ø§Ø¯ ØªÙ„Ø§Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø¯Ø¯ Ø¨Ø±Ø§ÛŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        """
        self.base_url = base_url
        self.config_dir = config_dir or os.path.join(BASE_DIR, 'config')

        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø¨Ø§ Ø§Ù…Ú©Ø§Ù† Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ
        self.max_threads = max_threads if max_threads is not None else CRAWLER_CONFIG['max_threads']
        self.initial_delay = initial_delay if initial_delay is not None else CRAWLER_CONFIG['politeness_delay']
        self.respect_robots = respect_robots if respect_robots is not None else CRAWLER_CONFIG['respect_robots']
        self.db_retry_attempts = database_retry_attempts

        # ÙØ±Ú©Ø§Ù†Ø³â€ŒÙ‡Ø§ÛŒ Ø®Ø²Ø´ (Ø¨Ù‡ Ø¯Ù‚ÛŒÙ‚Ù‡)
        self.crawl_frequency = {
            'initial': 1,  # Ù‡Ø± 1 Ø¯Ù‚ÛŒÙ‚Ù‡
            'active': 30,  # Ù‡Ø± 30 Ø¯Ù‚ÛŒÙ‚Ù‡
            'steady': 180,  # Ù‡Ø± 3 Ø³Ø§Ø¹Øª
            'maintenance': 1440  # Ù‡Ø± 24 Ø³Ø§Ø¹Øª (Ø±ÙˆØ²ÛŒ ÛŒÚ©Ø¨Ø§Ø±)
        }

        # Ø´Ù…Ø§Ø±Ø´Ú¯Ø±Ù‡Ø§ Ùˆ Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§
        self.urls_processed = 0
        self.urls_new_content = 0
        self.crawl_phase = 'initial'  # initial, active, steady, maintenance
        self.last_phase_change = datetime.now()

        # Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ù†Ù‚Ø·Ù‡ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ
        self.domain = self._extract_domain(base_url)
        self.checkpoint_file = os.path.join(self.config_dir, f"{self.domain}_smart_crawl_state.json")

        # Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ùˆ Ø³Ø§ÛŒØ± Ù…Ù†Ø§Ø¨Ø¹
        self.db_conn = None
        self.crawler = None
        self.structure_discovery = None
        self.content_extractor = None
        self.classifier = None
        self.storage_manager = None

        # ØµÙ Ú©Ø§Ø± Ùˆ ÙˆØ¶Ø¹ÛŒØª Ø®Ø²Ø´
        self.crawl_queue = queue.PriorityQueue()
        self.crawl_state = CrawlState(checkpoint_file=self.checkpoint_file)

        # Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ú©Ù†ØªØ±Ù„
        self.running = False
        self.stop_event = threading.Event()
        self.stats_lock = threading.Lock()

        # Ø¢Ù…Ø§Ø± Ùˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
        self.stats = {
            'startup_time': datetime.now(),
            'last_crawl_time': '',
            'total_urls_found': 0,
            'total_urls_processed': 0,
            'total_content_items': 0,
            'total_errors': 0,
            'pages_by_type': {},
            'current_phase': self.crawl_phase,
            'phase_transitions': []
        }

        # ØªÙ†Ø¸ÛŒÙ… Ú¯ÛŒØ±Ù†Ø¯Ù‡ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ§Ù† Ø¯Ø§Ø¯Ù† Ù…Ù†Ø§Ø³Ø¨
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

    def _extract_domain(self, url: str) -> str:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ù…Ù†Ù‡ Ø§Ø² URL

        Args:
            url: Ø¢Ø¯Ø±Ø³ URL

        Returns:
            Ø¯Ø§Ù…Ù†Ù‡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡
        """
        parsed = urlparse(url)
        return parsed.netloc.replace(".", "_")

    def _signal_handler(self, signum, frame):
        """
        Ù…Ø¯ÛŒØ±ÛŒØª Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ… Ø¹Ø§Ù…Ù„

        Args:
            signum: Ù†ÙˆØ¹ Ø³ÛŒÚ¯Ù†Ø§Ù„
            frame: ÙØ±ÛŒÙ… Ø§Ø¬Ø±Ø§ÛŒÛŒ
        """
        logger.info(f"Ø³ÛŒÚ¯Ù†Ø§Ù„ {signum} Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯. Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ‚Ù Ù…Ù†Ø§Ø³Ø¨...")
        self.stop()
        sys.exit(0)

    def verify_database_connection(self) -> bool:
        """
        Ø¨Ø±Ø±Ø³ÛŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡

        Returns:
            Ø¢ÛŒØ§ Ø§ØªØµØ§Ù„ Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ² Ø§Ø³ØªØŸ
        """
        logger.info("Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡...")

        for attempt in range(1, self.db_retry_attempts + 1):
            try:
                # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø§ØªØµØ§Ù„
                logger.info(f"ØªÙ„Ø§Ø´ {attempt} Ø§Ø² {self.db_retry_attempts} Ø¨Ø±Ø§ÛŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡")

                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² DatabaseConnection Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù¾Ø±ÙˆÚ˜Ù‡
                self.db_conn = DatabaseConnection()

                # Ø¨Ø±Ø±Ø³ÛŒ Ø§ØªØµØ§Ù„ Ø¨Ø§ Ø§Ø¬Ø±Ø§ÛŒ ÛŒÚ© Ú©ÙˆØ¦Ø±ÛŒ Ø³Ø§Ø¯Ù‡
                session = self.db_conn.get_session()
                session.execute("SELECT 1")
                session.close()

                logger.info("âœ… Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø±Ù‚Ø±Ø§Ø± Ø´Ø¯.")
                return True

            except Exception as e:
                logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡: {str(e)}")

                if attempt < self.db_retry_attempts:
                    # ØªØ£Ø®ÛŒØ± Ù†Ù…Ø§ÛŒÛŒ Ù‚Ø¨Ù„ Ø§Ø² ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯
                    delay = 2 ** attempt
                    logger.info(f"ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯ Ø¯Ø± {delay} Ø«Ø§Ù†ÛŒÙ‡...")
                    time.sleep(delay)
                else:
                    logger.critical("âŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ù¾Ø³ Ø§Ø² Ú†Ù†Ø¯ÛŒÙ† ØªÙ„Ø§Ø´ Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ù†Ø¨ÙˆØ¯.")
                    return False

    def verify_database_tables(self) -> bool:
        """
        Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø¬Ø¯Ø§ÙˆÙ„ Ù„Ø§Ø²Ù… Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ùˆ Ø§ÛŒØ¬Ø§Ø¯ Ø¢Ù†Ù‡Ø§ Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²

        Returns:
            Ø¢ÛŒØ§ Ø¬Ø¯Ø§ÙˆÙ„ Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù†Ø¯ ÛŒØ§ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ØŸ
        """
        logger.info("Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ Ø¬Ø¯Ø§ÙˆÙ„ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡...")

        try:
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø¬Ø¯Ø§ÙˆÙ„ Ø¨Ø§ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… ØªØ§Ø¨Ø¹ create_tables
            result = create_tables()

            if result:
                logger.info("âœ… Ø¬Ø¯Ø§ÙˆÙ„ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù†Ø¯.")
                return True
            else:
                logger.error("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ ÛŒØ§ Ø§ÛŒØ¬Ø§Ø¯ Ø¬Ø¯Ø§ÙˆÙ„ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡.")
                return False

        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ ÛŒØ§ Ø§ÛŒØ¬Ø§Ø¯ Ø¬Ø¯Ø§ÙˆÙ„: {str(e)}")
            return False

    def initialize_services(self) -> bool:
        """
        Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²

        Returns:
            Ø¢ÛŒØ§ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ² Ø¨ÙˆØ¯ØŸ
        """
        logger.info("Ø¯Ø± Ø­Ø§Ù„ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²...")

        try:
            # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø®Ø²Ø´Ú¯Ø±
            logger.info("Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø®Ø²Ø´Ú¯Ø±...")
            self.crawler = Crawler(
                base_url=self.base_url,
                config_dir=self.config_dir,
                max_threads=self.max_threads,
                politeness_delay=self.initial_delay,
                respect_robots=self.respect_robots
            )

            # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ø´Ù Ø³Ø§Ø®ØªØ§Ø±
            logger.info("Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ú©Ø´Ù Ø³Ø§Ø®ØªØ§Ø±...")
            self.structure_discovery = StructureDiscovery(self.base_url, self.config_dir)

            # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§
            logger.info("Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§...")
            self.content_extractor = ContentExtractor()

            # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ù†Ù†Ø¯Ù‡
            logger.info("Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ...")
            self.classifier = TextClassifier()

            # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ
            logger.info("Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ...")
            self.storage_manager = StorageManager()

            # Ø¨Ø±Ø±Ø³ÛŒ Ø¢Ù…Ø§Ø¯Ú¯ÛŒ Ø³ÛŒØ³ØªÙ…
            classifier_status = self.classifier.is_ready()
            if classifier_status.get('all_ready', False):
                logger.info("âœ… Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯.")
            else:
                logger.warning("âš ï¸ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¨Ù‡ Ø·ÙˆØ± Ú©Ø§Ù…Ù„ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù†Ø´Ø¯. Ø¨Ø±Ø®ÛŒ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ø§Ø´Ø¯.")

            logger.info("âœ… ØªÙ…Ø§Ù… Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯Ù†Ø¯.")
            return True

        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§: {str(e)}")
            return False

    def discover_site_structure(self, force=False) -> bool:
        """
        Ú©Ø´Ù Ø³Ø§Ø®ØªØ§Ø± ÙˆØ¨Ø³Ø§ÛŒØª

        Args:
            force: Ø¢ÛŒØ§ Ú©Ø´Ù Ø³Ø§Ø®ØªØ§Ø± Ø¨Ø§ ÙˆØ¬ÙˆØ¯ Ú©Ø´Ù Ù‚Ø¨Ù„ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ØŸ

        Returns:
            Ø¢ÛŒØ§ Ú©Ø´Ù Ø³Ø§Ø®ØªØ§Ø± Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ² Ø¨ÙˆØ¯ØŸ
        """
        logger.info("Ø¯Ø± Ø­Ø§Ù„ Ú©Ø´Ù Ø³Ø§Ø®ØªØ§Ø± ÙˆØ¨Ø³Ø§ÛŒØª...")

        try:
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø§Ú˜ÙˆÙ„ Ú©Ø´Ù Ø³Ø§Ø®ØªØ§Ø± Ù…ÙˆØ¬ÙˆØ¯
            success = self.structure_discovery.discover_structure(force=force)

            if success:
                logger.info("âœ… Ø³Ø§Ø®ØªØ§Ø± ÙˆØ¨Ø³Ø§ÛŒØª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ú©Ø´Ù Ø´Ø¯.")
            else:
                logger.warning("âš ï¸ Ú©Ø´Ù Ø³Ø§Ø®ØªØ§Ø± ÙˆØ¨Ø³Ø§ÛŒØª Ø¨Ø§ Ù…Ø´Ú©Ù„Ø§ØªÛŒ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯.")

            return success

        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ú©Ø´Ù Ø³Ø§Ø®ØªØ§Ø± ÙˆØ¨Ø³Ø§ÛŒØª: {str(e)}")
            return False

    def load_state(self) -> bool:
        """
        Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆØ¶Ø¹ÛŒØª Ù‚Ø¨Ù„ÛŒ Ø®Ø²Ø´ (Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯)

        Returns:
            Ø¢ÛŒØ§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ² Ø¨ÙˆØ¯ØŸ
        """
        logger.info("Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª Ù‚Ø¨Ù„ÛŒ Ø®Ø²Ø´...")

        try:
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„ ÙˆØ¶Ø¹ÛŒØª
            if os.path.exists(self.checkpoint_file):
                logger.info(f"ÙØ§ÛŒÙ„ ÙˆØ¶Ø¹ÛŒØª ÛŒØ§ÙØª Ø´Ø¯: {self.checkpoint_file}")

                # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆØ¶Ø¹ÛŒØª
                with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                    state = json.load(f)

                # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙˆØ¶Ø¹ÛŒØª Ø¬Ø§Ø±ÛŒ
                self.crawl_phase = state.get('crawl_phase', 'initial')
                self.urls_processed = state.get('urls_processed', 0)
                self.urls_new_content = state.get('urls_new_content', 0)

                # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø±
                if 'stats' in state:
                    temp_stats = state['stats']
                    if isinstance(temp_stats, dict):
                        # Ø­ÙØ¸ Ø¨Ø±Ø®ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø§ØµÙ„ÛŒ Ùˆ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¨Ù‚ÛŒÙ‡
                        temp_stats['startup_time'] = datetime.now()
                        self.stats.update(temp_stats)

                # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø²Ù…Ø§Ù† Ø¢Ø®Ø±ÛŒÙ† ØªØºÛŒÛŒØ± ÙØ§Ø²
                phase_change_str = state.get('last_phase_change')
                if phase_change_str:
                    try:
                        self.last_phase_change = datetime.fromisoformat(phase_change_str)
                    except (ValueError, TypeError):
                        self.last_phase_change = datetime.now()

                # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆØ¶Ø¹ÛŒØª Ø®Ø²Ø´
                self.crawl_state.load_checkpoint(self.checkpoint_file)

                logger.info(f"âœ… ÙˆØ¶Ø¹ÛŒØª Ø®Ø²Ø´ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯. ÙØ§Ø² ÙØ¹Ù„ÛŒ: {self.crawl_phase}, "
                            f"URLÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡: {self.urls_processed}")
                return True
            else:
                logger.info("ÙØ§ÛŒÙ„ ÙˆØ¶Ø¹ÛŒØª Ù‚Ø¨Ù„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø´Ø±ÙˆØ¹ Ø®Ø²Ø´ Ø§Ø² Ø§Ø¨ØªØ¯Ø§...")
                return False

        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆØ¶Ø¹ÛŒØª: {str(e)}")
            return False

    def save_state(self) -> bool:
        """
        Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ Ø®Ø²Ø´

        Returns:
            Ø¢ÛŒØ§ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ² Ø¨ÙˆØ¯ØŸ
        """
        logger.info("Ø¯Ø± Ø­Ø§Ù„ Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¶Ø¹ÛŒØª Ø®Ø²Ø´...")

        try:
            # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ¶Ø¹ÛŒØª
            state = {
                'crawl_phase': self.crawl_phase,
                'urls_processed': self.urls_processed,
                'urls_new_content': self.urls_new_content,
                'last_phase_change': self.last_phase_change.isoformat(),
                'stats': self.stats,
                'saved_at': datetime.now().isoformat()
            }

            # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²
            os.makedirs(os.path.dirname(self.checkpoint_file), exist_ok=True)

            # Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¶Ø¹ÛŒØª
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(state, f, ensure_ascii=False, indent=2)

            # Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¶Ø¹ÛŒØª Ø®Ø²Ø´
            self.crawl_state.save_checkpoint(self.checkpoint_file)

            logger.info(f"âœ… ÙˆØ¶Ø¹ÛŒØª Ø®Ø²Ø´ Ø¯Ø± {self.checkpoint_file} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
            return True

        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¶Ø¹ÛŒØª: {str(e)}")
            return False

    def update_crawl_phase(self) -> None:
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙØ§Ø² Ø®Ø²Ø´ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾ÛŒØ´Ø±ÙØª Ùˆ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ØªØ¹ÛŒÛŒÙ† Ø´Ø¯Ù‡"""
        current_time = datetime.now()
        time_in_phase = (current_time - self.last_phase_change).total_seconds() / 60  # Ø¨Ù‡ Ø¯Ù‚ÛŒÙ‚Ù‡

        with self.stats_lock:
            # Ù…Ù†Ø·Ù‚ ØªØºÛŒÛŒØ± ÙØ§Ø²
            if self.crawl_phase == 'initial':
                # Ù…Ø¹ÛŒØ§Ø± Ù¾ÛŒØ´Ø±ÙØª Ø§Ø² initial Ø¨Ù‡ active
                if self.urls_processed > 100 or time_in_phase > 60:  # 100 URL ÛŒØ§ 1 Ø³Ø§Ø¹Øª
                    self._change_phase('active')

            elif self.crawl_phase == 'active':
                # Ù…Ø¹ÛŒØ§Ø± Ù¾ÛŒØ´Ø±ÙØª Ø§Ø² active Ø¨Ù‡ steady
                if self.urls_processed > 1000 or time_in_phase > 240:  # 1000 URL ÛŒØ§ 4 Ø³Ø§Ø¹Øª
                    percentage_new = self.urls_new_content / max(1, self.urls_processed) * 100
                    if percentage_new < 20:  # Ú©Ù…ØªØ± Ø§Ø² 20Ùª Ù…Ø­ØªÙˆØ§ÛŒ Ø¬Ø¯ÛŒØ¯
                        self._change_phase('steady')

            elif self.crawl_phase == 'steady':
                # Ù…Ø¹ÛŒØ§Ø± Ù¾ÛŒØ´Ø±ÙØª Ø§Ø² steady Ø¨Ù‡ maintenance
                if self.urls_processed > 5000 or time_in_phase > 1440:  # 5000 URL ÛŒØ§ 24 Ø³Ø§Ø¹Øª
                    percentage_new = self.urls_new_content / max(1, self.urls_processed) * 100
                    if percentage_new < 5:  # Ú©Ù…ØªØ± Ø§Ø² 5Ùª Ù…Ø­ØªÙˆØ§ÛŒ Ø¬Ø¯ÛŒØ¯
                        self._change_phase('maintenance')

    def _change_phase(self, new_phase: str) -> None:
        """
        ØªØºÛŒÛŒØ± ÙØ§Ø² Ø®Ø²Ø´ Ùˆ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø±

        Args:
            new_phase: ÙØ§Ø² Ø¬Ø¯ÛŒØ¯
        """
        old_phase = self.crawl_phase
        self.crawl_phase = new_phase
        self.last_phase_change = datetime.now()

        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø±
        self.stats['current_phase'] = new_phase
        self.stats['phase_transitions'].append({
            'from': old_phase,
            'to': new_phase,
            'time': self.last_phase_change.isoformat(),
            'urls_processed': self.urls_processed,
            'urls_new_content': self.urls_new_content
        })

        # Ø±ÛŒØ³Øª Ø´Ù…Ø§Ø±Ø´Ú¯Ø±Ù‡Ø§
        self.urls_processed = 0
        self.urls_new_content = 0

        logger.info(f"ğŸ”„ ØªØºÛŒÛŒØ± ÙØ§Ø² Ø®Ø²Ø´ Ø§Ø² '{old_phase}' Ø¨Ù‡ '{new_phase}'")
        logger.info(f"â±ï¸ Ø²Ù…Ø§Ù† Ø®ÙˆØ§Ø¨ Ø¨ÛŒÙ† Ø®Ø²Ø´â€ŒÙ‡Ø§: {self.get_current_sleep_time()} Ø¯Ù‚ÛŒÙ‚Ù‡")

    def get_current_sleep_time(self) -> float:
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø²Ù…Ø§Ù† Ø®ÙˆØ§Ø¨ ÙØ¹Ù„ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙØ§Ø² Ø®Ø²Ø´

        Returns:
            Ø²Ù…Ø§Ù† Ø®ÙˆØ§Ø¨ Ø¨Ù‡ Ø¯Ù‚ÛŒÙ‚Ù‡
        """
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙØ±Ú©Ø§Ù†Ø³â€ŒÙ‡Ø§ÛŒ Ø§Ø² Ù¾ÛŒØ´ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡
        base_time = self.crawl_frequency[self.crawl_phase]

        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ú©Ù…ÛŒ ØªØµØ§Ø¯ÙÛŒâ€ŒØ³Ø§Ø²ÛŒ (Â±20Ùª) Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø±ÙØªØ§Ø± Ù‚Ø§Ø¨Ù„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        jitter = random.uniform(0.8, 1.2)
        return base_time * jitter

    def extract_and_store_content(self, url: str, html_content: str) -> bool:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ Ø§Ø² HTML Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø¢Ù† Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡

        Args:
            url: Ø¢Ø¯Ø±Ø³ URL ØµÙØ­Ù‡
            html_content: Ù…Ø­ØªÙˆØ§ÛŒ HTML ØµÙØ­Ù‡

        Returns:
            Ø¢ÛŒØ§ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ² Ø¨ÙˆØ¯ØŸ
        """
        logger.info(f"Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø­ØªÙˆØ§ÛŒ {url}...")

        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ContentExtractor
            extracted_data = self.content_extractor.extract(html_content, url)

            # Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø­ØªÙˆØ§
            classifier_status = self.classifier.is_ready()
            if classifier_status.get('all_ready', False):
                classification_result = self.classifier.classify_text(extracted_data['content'])

                # Ø§ÙØ²ÙˆØ¯Ù† Ù†ØªØ§ÛŒØ¬ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡
                if 'content_type' in classification_result:
                    content_type_info = classification_result.get('content_type', {})
                    extracted_data['content_type'] = content_type_info.get('content_type', 'other')

                if 'domains' in classification_result:
                    domains_info = classification_result.get('domains', {})
                    extracted_data['domains'] = domains_info.get('domains', [])
            else:
                # Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶
                extracted_data['content_type'] = 'other'
                extracted_data['domains'] = []

            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù…Ø­ØªÙˆØ§ÛŒ Ù…Ø´Ø§Ø¨Ù‡
            hash_value = ContentItem.calculate_similarity_hash(extracted_data['content'])
            existing_content = self.storage_manager.get_content_by_hash(hash_value)

            if existing_content:
                logger.info(f"Ù…Ø­ØªÙˆØ§ÛŒ Ù…Ø´Ø§Ø¨Ù‡ Ø¨Ø±Ø§ÛŒ {url} Ù‚Ø¨Ù„Ø§Ù‹ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª.")
                return False

            # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø­ØªÙˆØ§
            content_data = {
                'url': url,
                'title': extracted_data.get('title', ''),
                'content': extracted_data.get('content', ''),
                'content_type': extracted_data.get('content_type', 'other'),
                'meta_data': {
                    'date': extracted_data.get('date', ''),
                    'author': extracted_data.get('author', ''),
                    'entities': extracted_data.get('entities', {})
                }
            }

            stored_content = self.storage_manager.store_content(content_data)

            if stored_content:
                with self.stats_lock:
                    self.urls_new_content += 1
                    self.stats['total_content_items'] += 1

                    # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø± Ù†ÙˆØ¹ Ù…Ø­ØªÙˆØ§
                    content_type = content_data.get('content_type', 'other')
                    if 'pages_by_type' not in self.stats:
                        self.stats['pages_by_type'] = {}

                    self.stats['pages_by_type'][content_type] = self.stats['pages_by_type'].get(content_type, 0) + 1

                logger.info(f"âœ… Ù…Ø­ØªÙˆØ§ÛŒ {url} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
                return True
            else:
                logger.warning(f"âš ï¸ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø­ØªÙˆØ§ÛŒ {url} Ø¨Ø§ Ù…Ø´Ú©Ù„ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯.")
                return False

        except Exception as e:
            with self.stats_lock:
                self.stats['total_errors'] += 1

            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÛŒØ§ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø­ØªÙˆØ§ÛŒ {url}: {str(e)}")
            return False

    def process_url(self, url: str, depth: int = 0) -> Dict[str, Any]:
        """
        Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© URL: Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØªÙˆØ§ØŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ØŒ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø­ØªÙˆØ§

        Args:
            url: Ø¢Ø¯Ø±Ø³ URL Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´
            depth: Ø¹Ù…Ù‚ URL Ø¯Ø± Ú¯Ø±Ø§Ù Ø®Ø²Ø´

        Returns:
            Ù†ØªØ§ÛŒØ¬ Ù¾Ø±Ø¯Ø§Ø²Ø´
        """
        logger.info(f"Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ {url} (Ø¹Ù…Ù‚: {depth})...")

        # Ø¨Ø±Ø±Ø³ÛŒ Ù‚Ø¨Ù„Ø§Ù‹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù†
        if self.crawl_state.was_visited(url):
            logger.info(f"URL {url} Ù‚Ø¨Ù„Ø§Ù‹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ø§Ø³Øª.")
            return {'success': False, 'reason': 'already_visited'}

        try:
            # Ø¯Ø±ÛŒØ§ÙØª ØµÙØ­Ù‡
            response = self.crawler.request_manager.get(url)

            if not response.get('html'):
                logger.warning(f"âš ï¸ Ù…Ø­ØªÙˆØ§ÛŒ HTML Ø¨Ø±Ø§ÛŒ {url} Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                self.crawl_state.add_failed(url, error="No HTML content")
                return {'success': False, 'reason': 'no_html_content'}

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø­ØªÙˆØ§
            content_stored = self.extract_and_store_content(url, response['html'])

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
            links = extract_links(response['html'], url, internal_only=True)

            # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø±
            with self.stats_lock:
                self.urls_processed += 1
                self.stats['total_urls_processed'] += 1
                self.stats['total_urls_found'] += len(links)
                self.stats['last_crawl_time'] = datetime.now().isoformat()

            # Ø§ÙØ²ÙˆØ¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ ØµÙ Ø®Ø²Ø´
            new_links_added = 0
            for link in links:
                if not self.crawl_state.was_visited(link) and not self.crawl_state.is_in_progress(link):
                    # ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ù„ÛŒÙ†Ú©
                    pattern = self.structure_discovery.get_url_pattern(link)
                    job_type = 'page'

                    if pattern:
                        if pattern.is_list:
                            job_type = 'list'
                        elif pattern.is_detail:
                            job_type = 'detail'

                    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§ÙˆÙ„ÙˆÛŒØª
                    priority = self._calculate_url_priority(link, job_type, depth + 1)

                    # Ø§ÙØ²ÙˆØ¯Ù† Ø¨Ù‡ ØµÙ
                    self.crawler.add_job(link, depth=depth + 1, priority=priority, job_type=job_type)
                    new_links_added += 1

            # Ø«Ø¨Øª URL Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø´Ø¯Ù‡
            self.crawl_state.add_visited(url)

            logger.info(f"âœ… Ù¾Ø±Ø¯Ø§Ø²Ø´ {url} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯. {new_links_added} Ù„ÛŒÙ†Ú© Ø¬Ø¯ÛŒØ¯ ÛŒØ§ÙØª Ø´Ø¯.")

            return {
                'success': True,
                'url': url,
                'new_links': new_links_added,
                'content_stored': content_stored
            }

        except Exception as e:
            with self.stats_lock:
                self.stats['total_errors'] += 1

            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ {url}: {str(e)}")
            self.crawl_state.add_failed(url, error=str(e))

            return {'success': False, 'reason': str(e)}

    def _calculate_url_priority(self, url: str, job_type: str, depth: int) -> int:
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§ÙˆÙ„ÙˆÛŒØª URL Ø¨Ø±Ø§ÛŒ Ø®Ø²Ø´

        Args:
            url: Ø¢Ø¯Ø±Ø³ URL
            job_type: Ù†ÙˆØ¹ Ú©Ø§Ø± ('page', 'list', 'detail')
            depth: Ø¹Ù…Ù‚ URL Ø¯Ø± Ú¯Ø±Ø§Ù Ø®Ø²Ø´

        Returns:
            Ø§ÙˆÙ„ÙˆÛŒØª Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡ (Ù…Ù‚Ø§Ø¯ÛŒØ± Ú©Ù…ØªØ±ØŒ Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§ØªØ±)
        """
        # Ø§ÙˆÙ„ÙˆÛŒØª Ù¾Ø§ÛŒÙ‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ù…Ù‚
        priority = depth * 10

        # ØªÙ†Ø¸ÛŒÙ… Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ ØµÙØ­Ù‡
        if job_type == 'list':
            priority -= 20  # Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§ØªØ± Ø¨Ø±Ø§ÛŒ ØµÙØ­Ø§Øª Ù„ÛŒØ³ØªÛŒ
        elif job_type == 'detail':
            priority -= 10  # Ø§ÙˆÙ„ÙˆÛŒØª Ù…ØªÙˆØ³Ø· Ø¨Ø±Ø§ÛŒ ØµÙØ­Ø§Øª Ø¬Ø²Ø¦ÛŒØ§Øª

        # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ URL Ù…Ù‡Ù…
        important_patterns = [
            '/legal/', '/law/', '/cases/', '/judgments/', '/attorneys/',
            '/Ø­Ù‚ÙˆÙ‚ÛŒ/', '/Ù‚Ø§Ù†ÙˆÙ†/', '/Ù‚ÙˆØ§Ù†ÛŒÙ†/', '/Ù¾Ø±ÙˆÙ†Ø¯Ù‡/', '/Ø¯Ø§Ø¯Ú¯Ø§Ù‡/', '/ÙˆÚ©Ù„Ø§/'
        ]

        for pattern in important_patterns:
            if pattern in url:
                priority -= 5  # Ø§ÙØ²Ø§ÛŒØ´ Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø±Ø§ÛŒ URLâ€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…
                break

        return priority

    def crawl_worker(self):
        """ØªØ§Ø¨Ø¹ Ú©Ø§Ø±Ú¯Ø± Ø®Ø²Ø´ Ú©Ù‡ Ø¯Ø± Ù†Ø®â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯"""
        logger.info(f"Ù†Ø® Ú©Ø§Ø±Ú¯Ø± Ø®Ø²Ø´ Ø¢ØºØ§Ø² Ø¨Ù‡ Ú©Ø§Ø± Ú©Ø±Ø¯: {threading.current_thread().name}")

        while not self.stop_event.is_set():
            try:
                # Ø¯Ø±ÛŒØ§ÙØª Ú©Ø§Ø± Ø§Ø² ØµÙ Ø¨Ø§ Ø²Ù…Ø§Ù† Ø§Ù†ØªØ¸Ø§Ø±
                try:
                    job = self.crawler.job_queue.get(timeout=1)
                except queue.Empty:
                    continue

                # Ù¾Ø±Ø¯Ø§Ø²Ø´ URL
                self.process_url(job.url, job.depth)

                # Ø§Ø¹Ù„Ø§Ù… ØªÚ©Ù…ÛŒÙ„ Ú©Ø§Ø± Ø¨Ù‡ ØµÙ
                self.crawler.job_queue.task_done()

            except Exception as e:
                logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù†Ø® Ú©Ø§Ø±Ú¯Ø± Ø®Ø²Ø´: {str(e)}")

    def start_crawl_workers(self) -> bool:
        """
        Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù†Ø®â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ú¯Ø± Ø®Ø²Ø´

        Returns:
            Ø¢ÛŒØ§ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ² Ø¨ÙˆØ¯ØŸ
        """
        logger.info(f"Ø¯Ø± Ø­Ø§Ù„ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ {self.max_threads} Ù†Ø® Ú©Ø§Ø±Ú¯Ø± Ø®Ø²Ø´...")

        try:
            # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù†Ø®â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ú¯Ø±
            for i in range(self.max_threads):
                thread = threading.Thread(
                    target=self.crawl_worker,
                    name=f"CrawlWorker-{i + 1}",
                    daemon=True
                )
                thread.start()
                logger.info(f"Ù†Ø® Ú©Ø§Ø±Ú¯Ø± {i + 1} Ø¢ØºØ§Ø² Ø¨Ù‡ Ú©Ø§Ø± Ú©Ø±Ø¯.")

            logger.info(f"âœ… {self.max_threads} Ù†Ø® Ú©Ø§Ø±Ú¯Ø± Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯Ù†Ø¯.")
            return True

        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù†Ø®â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ú¯Ø±: {str(e)}")
            return False

    def run(self):
        """Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø®Ø²Ø´"""
        logger.info("ğŸš€ Ø¢ØºØ§Ø² Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø®Ø²Ø´...")

        # Ø¨Ø±Ø±Ø³ÛŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        if not self.verify_database_connection():
            logger.critical("Ø®Ø±ÙˆØ¬ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø¹Ø¯Ù… Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³.")
            return

        # Ø¨Ø±Ø±Ø³ÛŒ Ø¬Ø¯Ø§ÙˆÙ„ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        if not self.verify_database_tables():
            logger.critical("Ø®Ø±ÙˆØ¬ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ù…Ø´Ú©Ù„ Ø¯Ø± Ø¬Ø¯Ø§ÙˆÙ„ Ø¯ÛŒØªØ§Ø¨ÛŒØ³.")
            return

        # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§
        if not self.initialize_services():
            logger.critical("Ø®Ø±ÙˆØ¬ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ù…Ø´Ú©Ù„ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§.")
            return

        # Ú©Ø´Ù Ø³Ø§Ø®ØªØ§Ø± ÙˆØ¨Ø³Ø§ÛŒØª
        if not self.discover_site_structure():
            logger.warning("âš ï¸ Ú©Ø´Ù Ø³Ø§Ø®ØªØ§Ø± ÙˆØ¨Ø³Ø§ÛŒØª Ø¨Ø§ Ù…Ø´Ú©Ù„ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯. Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø§ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª...")

        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆØ¶Ø¹ÛŒØª Ù‚Ø¨Ù„ÛŒ
        self.load_state()

        # Ø§ÙØ²ÙˆØ¯Ù† URL Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ù‡ ØµÙ Ø®Ø²Ø´ (Ø§Ú¯Ø± ØµÙ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª)
        if self.crawler.job_queue.empty():
            logger.info(f"Ø§ÙØ²ÙˆØ¯Ù† URL Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ù‡ ØµÙ Ø®Ø²Ø´: {self.base_url}")
            self.crawler.add_job(self.base_url, depth=0, job_type='page')

        # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù†Ø®â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ú¯Ø±
        if not self.start_crawl_workers():
            logger.critical("Ø®Ø±ÙˆØ¬ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ù…Ø´Ú©Ù„ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù†Ø®â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ú¯Ø±.")
            return

        # Ø¹Ù„Ø§Ù…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø´Ø±ÙˆØ¹ Ø§Ø¬Ø±Ø§
        self.running = True

        try:
            # Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡
            logger.info("â±ï¸ ÙˆØ±ÙˆØ¯ Ø¨Ù‡ Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø®Ø²Ø´...")

            while self.running and not self.stop_event.is_set():
                # Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª
                current_queue_size = self.crawler.job_queue.qsize()
                stats = self.crawl_state.get_stats()

                logger.info(f"ğŸ“Š ÙˆØ¶Ø¹ÛŒØª Ø®Ø²Ø´ - ÙØ§Ø²: {self.crawl_phase}, "
                            f"ØµÙ: {current_queue_size}, "
                            f"URLÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡: {stats['successful_urls']}, "
                            f"Ù…Ø­ØªÙˆØ§ÛŒ Ø¬Ø¯ÛŒØ¯: {self.urls_new_content}")

                # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙØ§Ø² Ø®Ø²Ø´
                self.update_crawl_phase()

                # Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ
                self.save_state()

                # Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø®ÙˆØ§Ø¨
                sleep_minutes = self.get_current_sleep_time()
                logger.info(f"ğŸ’¤ Ø§Ø³ØªØ±Ø§Ø­Øª Ø¨Ø±Ø§ÛŒ {sleep_minutes:.2f} Ø¯Ù‚ÛŒÙ‚Ù‡...")

                # Ø®ÙˆØ§Ø¨ Ø¨Ø§ Ø§Ù…Ú©Ø§Ù† ÙˆÙ‚ÙÙ‡
                for _ in range(int(sleep_minutes * 60)):
                    if self.stop_event.is_set():
                        break
                    time.sleep(1)

                # Ø¨Ø±Ø±Ø³ÛŒ Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯Ù† ØµÙ
                if current_queue_size == 0:
                    logger.info("ØµÙ Ø®Ø²Ø´ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª. Ø§ÙØ²ÙˆØ¯Ù† URL Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ ØªØºÛŒÛŒØ±Ø§Øª...")
                    self.crawler.add_job(self.base_url, depth=0, job_type='page')

            logger.info("Ø®Ø±ÙˆØ¬ Ø§Ø² Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø®Ø²Ø´.")

        except KeyboardInterrupt:
            logger.info("Ø¯Ø±ÛŒØ§ÙØª Ø³ÛŒÚ¯Ù†Ø§Ù„ ØªÙˆÙ‚Ù Ø§Ø² Ú©Ø§Ø±Ø¨Ø±...")
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø®Ø²Ø´: {str(e)}")
        finally:
            self.stop()

    def stop(self):
        """ØªÙˆÙ‚Ù Ù…Ù†Ø§Ø³Ø¨ ØªÙ…Ø§Ù… ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§"""
        if not self.running:
            return

        logger.info("Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ‚Ù Ù…Ù†Ø§Ø³Ø¨ Ù…Ø¯ÛŒØ±ÛŒØª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø®Ø²Ø´...")

        # Ø¹Ù„Ø§Ù…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ‚Ù
        self.stop_event.set()
        self.running = False

        # Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¶Ø¹ÛŒØª Ù†Ù‡Ø§ÛŒÛŒ
        self.save_state()

        # ØªÙˆÙ‚Ù Ø®Ø²Ø´Ú¯Ø±
        if self.crawler:
            self.crawler.stop(wait=True, save_checkpoint=True)

        logger.info("âœ… Ù…Ø¯ÛŒØ±ÛŒØª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø®Ø²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù…ØªÙˆÙ‚Ù Ø´Ø¯.")


def parse_arguments():
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø· ÙØ±Ù…Ø§Ù†"""
    parser = argparse.ArgumentParser(
        description="Ù…Ø¯ÛŒØ±ÛŒØª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø®Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ù‚ÙˆÙ‚ÛŒ"
    )
    parser.add_argument(
        "--base-url", type=str,
        default=os.getenv("INITIAL_URL", "https://www.bonyadvokala.com/"),
        help="Ø¢Ø¯Ø±Ø³ Ù¾Ø§ÛŒÙ‡ ÙˆØ¨Ø³Ø§ÛŒØª Ù‡Ø¯Ù"
    )
    parser.add_argument(
        "--max-threads", type=int,
        default=int(os.getenv("MAX_THREADS", "4")),
        help="Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ù†Ø®â€ŒÙ‡Ø§ÛŒ Ù‡Ù…Ø²Ù…Ø§Ù†"
    )
    parser.add_argument(
        "--delay", type=float,
        default=float(os.getenv("CRAWL_DELAY", "1.0")),
        help="ØªØ£Ø®ÛŒØ± Ø§ÙˆÙ„ÛŒÙ‡ Ø¨ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ (Ø«Ø§Ù†ÛŒÙ‡)"
    )
    parser.add_argument(
        "--no-robots", action="store_true",
        help="Ø¹Ø¯Ù… Ø±Ø¹Ø§ÛŒØª Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ robots.txt"
    )
    return parser.parse_args()


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡"""
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§
    args = parse_arguments()

    # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯ÛŒØ±ÛŒØª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø®Ø²Ø´
    crawler_manager = SmartCrawlManager(
        base_url=args.base_url,
        max_threads=args.max_threads,
        initial_delay=args.delay,
        respect_robots=not args.no_robots
    )

    # Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø®Ø²Ø´
    crawler_manager.run()


if __name__ == "__main__":
    main()